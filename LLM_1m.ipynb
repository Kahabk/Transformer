{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14663ca-4514-42f9-99f6-d91ea2e7820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "# Custom Collate Function to Pad Sequences\n",
    "def custom_collate_fn(batch, pad_token_id):\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=pad_token_id\n",
    "    )\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=pad_token_id\n",
    "    )\n",
    "    return inputs_padded, labels_padded\n",
    "\n",
    "# RoPE Function \n",
    "def apply_rope(x, position_ids, base=10000):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "    dim_indices = torch.arange(0, head_dim // 2, dtype=torch.float, device=x.device)\n",
    "    theta = base ** (-2 * dim_indices / head_dim)\n",
    "    angles = position_ids[:, None, :, None] * theta[None, None, None, :]\n",
    "    sin_angles = torch.sin(angles)\n",
    "    cos_angles = torch.cos(angles)\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    rotated_even = x_even * cos_angles - x_odd * sin_angles\n",
    "    rotated_odd = x_even * sin_angles + x_odd * cos_angles\n",
    "    rotated = torch.stack((rotated_even, rotated_odd), dim=-1).reshape(x.shape)\n",
    "    return rotated\n",
    "\n",
    "# MultiHeadAttention \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        assert self.d_k % 2 == 0, \"d_k must be even for RoPE\"\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.float, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        Q = apply_rope(Q, position_ids)\n",
    "        K = apply_rope(K, position_ids)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "            mask = mask[None, None, :, :]\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return self.W_o(attn_output)\n",
    "\n",
    "# TransformerDecoderBlock \n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_hidden_dim, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_input = self.norm1(x)\n",
    "        attn_output = self.attention(attn_input)\n",
    "        x = x + attn_output\n",
    "        ffn_input = self.norm2(x)\n",
    "        ffn_output = self.ffn(ffn_input)\n",
    "        x = x + ffn_output\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_blocks, ff_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerDecoderBlock(d_model, num_heads, ff_hidden_dim) for _ in range(num_blocks)])\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        convo = self.data[idx]['conversation']\n",
    "        if len(convo) < 2 or convo[0]['role'] != 'user' or convo[1]['role'] != 'assistant':\n",
    "            next_idx = (idx + 1) % len(self.data)\n",
    "            if next_idx == idx:\n",
    "                raise ValueError(\"No valid conversations found in dataset\")\n",
    "            return self.__getitem__(next_idx)\n",
    "        question = convo[0]['content']\n",
    "        answer = convo[1]['content']\n",
    "        qa_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "        tokens = self.tokenizer.encode(\n",
    "            qa_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze()\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 50257\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_blocks = 4\n",
    "ff_hidden_dim = 1024\n",
    "max_len = 64\n",
    "batch_size = 2\n",
    "num_epochs = 100  # Increased for better training\n",
    "learning_rate = 5e-5\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "# Load Dataset (1,000 examples)\n",
    "dataset = load_dataset(\"lmsys/lmsys-chat-1m\", split='train').select(range(1000))\n",
    "\n",
    "# Filter valid conversations\n",
    "def is_valid_conversation(example):\n",
    "    return (\n",
    "        'conversation' in example and\n",
    "        len(example['conversation']) >= 2 and\n",
    "        example['conversation'][0]['role'] == 'user' and\n",
    "        example['conversation'][1]['role'] == 'assistant'\n",
    "    )\n",
    "\n",
    "valid_data = dataset.filter(is_valid_conversation)\n",
    "print(f\"Number of valid conversations: {len(valid_data)}\")\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset = QADataset(valid_data, tokenizer, max_len)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: custom_collate_fn(batch, tokenizer.pad_token_id)\n",
    ")\n",
    "\n",
    "# Model, Optimizer, Loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT(vocab_size, d_model, num_heads, num_blocks, ff_hidden_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "total_loss = 0\n",
    "steps = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        if steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        del inputs, labels, outputs, loss\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss / steps:.4f}\")\n",
    "\n",
    "# Improved Generation with Top-k Sampling\n",
    "def generate_response(question, max_new_tokens=50, top_k=50, temperature=0.7):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(f\"Question: {question}\\nAnswer:\", return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # Top-k sampling\n",
    "            top_k_probs, top_k_indices = torch.topk(F.softmax(logits, dim=-1), top_k, dim=-1)\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(-1, next_token)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    response = tokenizer.decode(input_ids[0])\n",
    "    del input_ids, outputs, logits, top_k_probs, top_k_indices, next_token\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    return response\n",
    "\n",
    "print(\"Sample Generation:\", generate_response(\"What is AI?\"))\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"gpt_like_qa_model.pth\")\n",
    "print(\"Model saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
